## 追一项目



#### 坐席辅助机器人

**背景介绍**
	服务于银行、金融行业坐席客服的实时智能助理机器人，坐席语音进线后，根据实时的语音内容，智能质检后提供相应业务的话术推荐、固定流程指引(办卡、销户等)，违规提醒等。达到提高坐席产能，缩短坐席上岗周期，全方位提升客户服务能力上限。

**服务架构**

统一网关：路由转发和登陆校验
接入服务：通话接入适配
管理服务：业务规则配置管理，事后数据分析
质检服务：实时语音转义质检，坐席提醒。(讲相关的流程)
	1（略过）坐席登陆，网关通知分发服务上线，分发服务向宇高等录音服务订阅该坐席的语音数据；网关通知质检服务坐席上线。坐席进线后创建一个ws连接，用于接受后续的信息推送。
	2 分发接受udp包推送，根据会话id，通过redis队列推送给质检服务
	3 质检服务接受到语音信息，通过grpc调用asr将语音转化为文字信息。拿到对应文字信息后，根据开始信息创建一条会话独有的线程，用来处理该通会话所有的质检处理。
	质检过程中，每有新一句的会话过来，就针对话术推荐、流程导航、质检提醒等做一次质检操作，即将会话上下文和定时从管理服务拉去的规则发送给挖掘引擎进行质检。质检结束后，将结果进行组装后，推送给坐席端。
	会话结束后，将整通会话相关的信息，推送到kafaka，然后管理端和ddp会消费对应的信息。ddp主要是做数据录入，管理端会保存会话历史信息，同时会分析和处理

![架构流程](https://s3.bmp.ovh/imgs/2022/05/17/9f8dd52d87ba24db.png)



项目优化内容**
**1. 管理服务假死**
curl请求无反应，日志不刷新，jstat命令查看gc状态一直在频繁gc。
dump出日志文件，mat(mem analy tool)工具分析查看是假死。
查看的过程
命令：jstat gc pid查看gc信息

**解决方式**：

1 mybatis设置查询结果缓存，```@Options(resultSetType = ResultSetType.FORWARD_ONLY, fetchSize = 1000)```
2 以及超过1000条就flush到excel上一次。

**2. 接入/分发服务优化**

现象，坐席会话信息丢失。
解决过程
1-查看日志，报udp报队列满的错误。tcpdump抓包，发现和发送端的数量差别巨大，确认丢包问题。
2-接入服务语音包处理加切面日志打印处理时间，发现redis请求耗时比较大。背景：每个包需要频繁请求redis获取里面的映射信息。
3-上redis服务端查看，redis命令执行在100微秒就已经执行完毕，剩下的都卡在连接池上了。200路的并发，redis是串行读取的。

这里开始给出解决方案，将redis中的映射关系，使用本地缓存的方式保存，然后同步一份在redis缓存里。本地没有的时候才去缓存中获取。
再次tcpdump，发现和发送端数量一致。

查看命令执行在100us就已经执行完，但是获取时间很长，设置的5秒超时经常超时。---这个是质检服务的优化项
通过查看redis客户端耗时很短，java调用很长。
redis是串行的请求，1s内200个连接，平均每个5ms.

redis集群，3主3从，6个节点

3**宇高的udp高有设置一个缓冲区**，我们通过日志打印顺序和时间，一直乱序。发现有一个缓冲区，满缓冲大小才发。对方不承认，后面换了懂技术的才承认，我们自己修改对方的xml配置文件后，缓解乱序问题。

因为这个缓冲区存在，10个包一起发送，无法保证接受顺序就是发送顺序。
后面在每5个包里，在将语音包发送给asr的过程中做了一个排序，用于进一步优化缓解乱序问题。

如果换一种协议，使用grpc来实现会更好。
为什么不用http。每一个包都要三次握手，大量的接受压力。大量消耗客户端的端口。四次挥手的时候会有一个端口等待过程，这个时间是不可用的。
tcp

4**质检服务**
redis
讲中信的问题，各种加锁来实现，各种线程消耗，问题排查困难。
优化后的结果讲标品的实现。
接入服务---grpc发送--->固定到质检客户端。
在高并发情况下，多次操作redis会导致接口超时。需要在redis中获取坐席的状态。
改成lua脚本来操作简单的业务逻辑。

之前直接将规则数据推送给挖掘引擎，优化后只推送规则。

**性能瓶颈**在哪里：
语音200路，文本300.可以支持扩展。双节点部署。
瓶颈在于挖掘引擎，那边反馈比较慢。规则比较多。

还有哪些可以优化。
服务端ng做的负载-应该在客户端做负载，使用微服务的思想



todo 下面待整理

#### 项目背景

**分发服务**：

坐席登陆时，订阅录音，分配对应的中控服务端(用于转译)。将session登陆状态放在redis中，每次有新包过来时，需要获取和中控的绑定信息。然后将语音包发送给中控服务。

缺点：
大概20ms一个包，读取redis的操作阻塞的处理速度，处理不够快就会阻塞丢包。
需要回写一个坐席信息。

优化后设计：
每次坐席登陆后，将映射信息通过redis广播同步到其他服务，直接从本地缓存中获取。(会导致头几个包丢失，可能会影响第一句的语音转译效果)


**还可以优化的点**：
在标品项目中有体现，接入服务只做通话数据的接入。用于适配不同的录音接入。



**质检服务**：
一个key，包含所有的数据。
获取到一个msg时，检查redis信息判断能否进行质检。抢占一个redis锁，key信息(busId+sessionId+chatId)，然后开始质检
执行完后，会在redis中将下一句设置为可质检状态。
所以线程获取到msg，不能质检的时候sleep后重试

缺点：
1一通会话多个语句可能在多台服务的多个线程中进行。也不利于日志排查。
2只有上一句执行完了下一句才能执行，线程阻塞造成性能问题。
3redis取消息用的阻塞取出一句信息，容易卡性能。
4质检时将会话数据和全部规则数据推送给挖掘引擎，http消耗巨大

优化后设计：
将开始标志信息单独放在一个redis队列，数据队列根据sessionId分组放在个字单独的redis队列。
获取到标志信息时，在当前服务上创建唯一的线程去处理指定seesionId的数据。
只将质检规则id推送给挖掘引擎，在管理服务那边更新规则后将规则数据同步到挖掘引擎。

redis获取消息时，不再只取单句，而是一次性取出现存队列的的全部。

**还可以优化的点**
用于处理该通会话的上下文比较巨大，推送给挖掘引擎的时候会比较消耗性能。
上下文保存在线程私有变量中,并且有很多重复的东西，这里还需要继续优化。

#### grpc架构







**

